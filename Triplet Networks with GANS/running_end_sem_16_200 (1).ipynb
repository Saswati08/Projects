{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "running_end_sem_16_200.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgOTUWyHjRFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXYSGZcujani",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd drive/My Drive/DL_endsem_improvedGANs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWGmrKpckm56",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXxSN-wNjJBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd Colab Notebooks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6tRBFSFjOUR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoHYR0I8k7Z6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install tensorboardx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yF0jGQFqjNe0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rh3iTSbJhzhA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function \n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from functional import *\n",
        "from torch.utils.data import DataLoader,TensorDataset\n",
        "import sys\n",
        "import argparse\n",
        "# from Datasets import *\n",
        "import pdb\n",
        "import tensorboardX\n",
        "import os\n",
        "from itertools import cycle\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.metrics import average_precision_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8mxh3fxuqzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "\n",
        "def MnistLabel(class_num, ntriplets):\n",
        "    raw_dataset = datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                   ]))\n",
        "    class_tot = [0] * 10\n",
        "    data = []\n",
        "    labels = []\n",
        "    positive_tot = 0\n",
        "    tot = 0\n",
        "    \n",
        "    for i in range(raw_dataset.__len__()):\n",
        "        datum, label = raw_dataset.__getitem__(i)\n",
        "        if class_tot[label] < class_num:\n",
        "            data.append(datum.numpy())\n",
        "            labels.append(label)\n",
        "            class_tot[label] += 1\n",
        "            tot += 1\n",
        "            if tot >= 10 * class_num:\n",
        "                break\n",
        "    labels = np.array(labels)\n",
        "    triplet_data = []\n",
        "    triplet_label = []\n",
        "    triplets = []\n",
        "    for i in range(ntriplets):\n",
        "      \n",
        "      class_idx = np.random.choice([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 1)[0]\n",
        "      \n",
        "\n",
        "      a = np.random.choice(np.where(labels==class_idx)[0], 1)[0]\n",
        "      b = np.random.choice(np.where(labels==class_idx)[0], 1)[0]\n",
        "      while (a-b) == 0:\n",
        "          b = np.random.choice(np.where(labels==class_idx)[0], 1)[0]\n",
        "      c = np.random.choice(np.where(labels!=class_idx)[0], 1)[0]\n",
        "      triplets.append([a, b, c])\n",
        "    # print(labels[triplets[0][0]], labels[triplets[0][1]], labels[triplets[0][2]])\n",
        "    # print(triplets[0][0])\n",
        "    for i in range(0, ntriplets, 100):\n",
        "      k = i\n",
        "      for j in range(k, k + 100):\n",
        "          triplet_data.append(data[triplets[j][0]])\n",
        "          triplet_label.append(labels[triplets[j][0]])\n",
        "      for j in range(k, k + 100):\n",
        "          triplet_data.append(data[triplets[j][1]])\n",
        "          triplet_label.append(labels[triplets[j][1]])\n",
        "      for j in range(k, k + 100):\n",
        "          triplet_data.append(data[triplets[j][2]])\n",
        "          triplet_label.append(labels[triplets[j][2]])\n",
        "    # print(len(triplet_data), len(triplet_label))\n",
        "    # print(triplet_label[301], triplet_label[1])\n",
        "    # print(triplet_label[401], triplet_label[101])\n",
        "    # print(triplet_label[501], triplet_label[201])\n",
        "    triplet_data = np.array(triplet_data)\n",
        "    # print(triplet_data.dtype)\n",
        "    triplet_label = np.array(triplet_label)\n",
        "    return TensorDataset(torch.FloatTensor(triplet_data), torch.LongTensor(triplet_label))\n",
        "\n",
        "\n",
        "def MnistUnlabel():\n",
        "    raw_dataset = datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                   ]))\n",
        "    return raw_dataset\n",
        "def MnistTest():\n",
        "    return datasets.MNIST('../data', train=False, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                   ]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbRBpUXpldeM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# architecture taken from @https://github.com/Sleepychord/ImprovedGAN-pytorch\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim = 28 ** 2, output_dim = 16):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.layers = torch.nn.ModuleList([\n",
        "            LinearWeightNorm(input_dim, 1000),\n",
        "            LinearWeightNorm(1000, 500),\n",
        "            LinearWeightNorm(500, 250),\n",
        "            LinearWeightNorm(250, 250),\n",
        "            LinearWeightNorm(250, 250)]\n",
        "        )\n",
        "        self.final = LinearWeightNorm(250, output_dim, weight_scale=1)\n",
        "\n",
        "\n",
        "    def forward(self, x, feature = False):\n",
        "        x = x.view(-1, self.input_dim).cuda()\n",
        "        noise = torch.randn(x.size()) * 0.3 if self.training else torch.Tensor([0])\n",
        "        noise = noise.cuda()\n",
        "        x = x + Variable(noise)\n",
        "        for i in range(len(self.layers)):\n",
        "            m = self.layers[i]\n",
        "            x_f = F.relu(m(x))\n",
        "            noise = torch.randn(x_f.size()) * 0.5 if self.training else torch.Tensor([0])\n",
        "            noise = noise.cuda()\n",
        "            x = (x_f + Variable(noise))\n",
        "        if feature:\n",
        "            return x_f, self.final(x)\n",
        "        return self.final(x)\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim, output_dim = 28 ** 2):\n",
        "        super(Generator, self).__init__()\n",
        "        self.z_dim = z_dim\n",
        "        self.fc1 = nn.Linear(z_dim, 500, bias = False)\n",
        "        self.bn1 = nn.BatchNorm1d(500, affine = False, eps=1e-6, momentum = 0.5)\n",
        "        self.fc2 = nn.Linear(500, 500, bias = False)\n",
        "        self.bn2 = nn.BatchNorm1d(500, affine = False, eps=1e-6, momentum = 0.5)\n",
        "        self.fc3 = LinearWeightNorm(500, output_dim, weight_scale = 1)\n",
        "        self.bn1_b = Parameter(torch.zeros(500))\n",
        "        self.bn2_b = Parameter(torch.zeros(500))\n",
        "        nn.init.xavier_uniform(self.fc1.weight)\n",
        "        nn.init.xavier_uniform(self.fc2.weight)\n",
        "\n",
        "    def forward(self, batch_size):\n",
        "        x = Variable(torch.rand(batch_size, self.z_dim), requires_grad = False, volatile = not self.training)\n",
        "        \n",
        "        x = x.cuda()\n",
        "        x = F.softplus(self.bn1(self.fc1(x)) + self.bn1_b)\n",
        "        x = F.softplus(self.bn2(self.fc2(x)) + self.bn2_b)\n",
        "        x = F.softplus(self.fc3(x))\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXmD3f8Ar-xQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_triplets(prediction,size):\n",
        "      a = prediction[0 : size] # query case (positive)\n",
        "      b = prediction[size : 2 * size] # positive case\n",
        "      c = prediction[2 * size : 3 * size] # negative\n",
        "\n",
        "      return a,b,c\n",
        "\n",
        "def loss_labeled(a,b,c):\n",
        "        n_plus = torch.sqrt(torch.sum((a - b)**2, axis=1))\n",
        "        n_minus = torch.sqrt(torch.sum((a - c)**2, axis=1))\n",
        "        # print(n_plus.size(), n_minus.size())\n",
        "        z = torch.cat((torch.unsqueeze(n_minus, 1), torch.unsqueeze(n_plus, 1)), 1)\n",
        "        z = log_sum_exp(z, axis=1)\n",
        "        loss_lab = -torch.mean(n_minus) + torch.mean(z)\n",
        "        return loss_lab\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXtEhWsuZTBn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class ImprovedGAN(object):\n",
        "    def __init__(self, G, D, labeled, unlabeled, test, args):\n",
        "        # if os.path.exists(args.savedir):\n",
        "        #     print('Loading model from ' + args.savedir)\n",
        "        #     self.G = torch.load(os.path.join(args.savedir, 'G.pkl'))\n",
        "        #     self.D = torch.load(os.path.join(args.savedir, 'D.pkl'))\n",
        "        # else:\n",
        "        #     os.makedirs(args.savedir)\n",
        "        #     self.G = G\n",
        "        #     self.D = D\n",
        "        #     torch.save(self.G, os.path.join(args.savedir, 'G.pkl'))\n",
        "        #     torch.save(self.D, os.path.join(args.savedir, 'D.pkl'))\n",
        "        # self.writer = tensorboardX.SummaryWriter(log_dir=args.logdir)\n",
        "        self.G = G\n",
        "        self.D = D\n",
        "        if args.cuda:\n",
        "            self.G.cuda()\n",
        "            self.D.cuda()\n",
        "        self.labeled = labeled\n",
        "        self.unlabeled = unlabeled\n",
        "        self.test = test\n",
        "        self.Doptim = optim.Adam(self.D.parameters(), lr=args.lr, betas= (args.momentum, 0.999))\n",
        "        self.Goptim = optim.Adam(self.G.parameters(), lr=args.lr, betas = (args.momentum,0.999))\n",
        "        self.args = args\n",
        "    \n",
        "\n",
        "    def trainD(self, x_label, y, x_unlabel):\n",
        "        \n",
        "        x_label = Variable(x_label).cuda()\n",
        "        x_unlabel = Variable(x_unlabel).cuda()\n",
        "        y = Variable(y).cuda()\n",
        "        #  y = Variable(y, requires_grad = False)\n",
        "        \n",
        "        output_label = self.D(x_label)\n",
        "        output_unlabel = self.D(x_unlabel) \n",
        "        output_fake_G = self.G(x_unlabel.size()[0]).view(x_unlabel.size())\n",
        "        output_fake = self.D(output_fake_G)  \n",
        "        \n",
        "        \n",
        "\n",
        "        a_lab, b_lab, c_lab = get_triplets(output_label, 100)\n",
        "        loss_triplet = loss_labeled(a_lab, b_lab, c_lab)\n",
        "           \n",
        "        loss_unsupervised = -0.5 * torch.mean(log_sum_exp(output_unlabel)) + 0.5 * torch.mean(F.softplus(log_sum_exp(output_unlabel))) + 0.5 * torch.mean(F.softplus(log_sum_exp(output_fake)))\n",
        "\n",
        "        loss = loss_unsupervised + loss_triplet\n",
        "        \n",
        "        self.Doptim.zero_grad()\n",
        "        loss.backward()\n",
        "        self.Doptim.step()\n",
        "        return loss.detach().cpu().numpy(), loss_unsupervised.detach().cpu().numpy(), loss_triplet.detach().cpu().numpy()\n",
        "    \n",
        "    def trainG(self, x_unlabel):\n",
        "        \n",
        "        fake = self.G(x_unlabel.size()[0]).view(x_unlabel.size())\n",
        "        mom_gen, output_fake = self.D(fake, feature=True)\n",
        "        mom_unlabel, _ = self.D(Variable(x_unlabel), feature=True)\n",
        "        mom_gen = torch.mean(mom_gen, dim = 0)\n",
        "        mom_unlabel = torch.mean(mom_unlabel, dim = 0)\n",
        "        \n",
        "        loss_fm = torch.mean(torch.square(torch.mean(mom_gen, axis = 0))) - torch.mean(mom_unlabel, axis = 0)\n",
        "        loss = loss_fm \n",
        "        self.Goptim.zero_grad()\n",
        "        self.Doptim.zero_grad()\n",
        "        loss.backward()\n",
        "        self.Goptim.step()\n",
        "        return loss.detach().cpu().numpy()\n",
        "\n",
        "    def train(self):\n",
        "      if not os.path.exists('checkpoints_16_200_final'):\n",
        "        os.makedirs('checkpoints_16_200_final')\n",
        "     \n",
        "      if args.load_saved == True:\n",
        "        self.G.load_state_dict(torch.load(os.path.join('checkpoints_16_200_final', args.generator_save)))\n",
        "        self.D.load_state_dict(torch.load(os.path.join('checkpoints_16_200_final', args.discriminator_save)))\n",
        "      loss_g_list = []\n",
        "      loss_triplet_list = []\n",
        "      loss_disc_list = []\n",
        "      loss_unsuper_list = []\n",
        "      self.G.train()\n",
        "      self.D.train()\n",
        "      \n",
        "      for epoch in range(self.args.epochs):\n",
        "          unlabel_loader1 = DataLoader(self.unlabeled, batch_size = self.args.batch_size, shuffle = True, drop_last = True, num_workers = 4)\n",
        "          unlabel_loader2 = DataLoader(self.unlabeled, batch_size = self.args.batch_size, shuffle = True, drop_last = True, num_workers = 4).__iter__()\n",
        "          label_loader = DataLoader(MnistLabel(20, 60000), batch_size = 300, shuffle = False, drop_last=True, num_workers = 4).__iter__()\n",
        "          loss_disc = 0\n",
        "          loss_gen = 0\n",
        "          batch_num = 0\n",
        "          loss_triplet = 0\n",
        "          loss_unsuper = 0\n",
        "          \n",
        "          for (unlabel1, label1) in unlabel_loader1:\n",
        "              \n",
        "              unlabel2, label2 = next(unlabel_loader2)\n",
        "              x, y = next(label_loader)\n",
        "              # if batch_num == 0:\n",
        "              # print(batch_num, x.size(), y[0], y[34], y[98])\n",
        "              x = x.cuda()\n",
        "              y = y.cuda()\n",
        "              unlabel1 = unlabel1.cuda()\n",
        "              unlabel2 = unlabel2.cuda()\n",
        "\n",
        "              # print(x.size(), y.size(), unlabel1.size())\n",
        "              ld, lus, ltr = self.trainD(x, y, unlabel1)\n",
        "              \n",
        "              lg = self.trainG(unlabel2)\n",
        "              if epoch > 1 and lg > 1:\n",
        "                  lg = self.trainG(unlabel2)\n",
        "              \n",
        "              loss_disc += ld\n",
        "              loss_gen += lg\n",
        "              loss_unsuper += lus\n",
        "              loss_triplet += ltr\n",
        "              batch_num += 1\n",
        "              \n",
        "          \n",
        "          loss_gen = loss_gen / batch_num\n",
        "          loss_disc = loss_disc / batch_num\n",
        "          loss_unsuper = loss_unsuper / batch_num\n",
        "          loss_triplet = loss_triplet / batch_num \n",
        "          print(epoch, loss_gen, loss_disc, loss_unsuper, loss_triplet)\n",
        "          loss_g_list.append(loss_gen)\n",
        "          loss_triplet_list.append(loss_triplet)\n",
        "          loss_unsuper_list.append(loss_unsuper)\n",
        "          loss_disc_list.append(loss_disc)\n",
        "          \n",
        "          if (epoch + 1) % self.args.eval_interval == 0 and args.save == True:\n",
        "              \n",
        "              torch.save(self.G.state_dict(), os.path.join('checkpoints_16_200_final', args.generator_save))\n",
        "              torch.save(self.D.state_dict(), os.path.join('checkpoints_16_200_final', args.discriminator_save))\n",
        "          if epoch == 1:\n",
        "              torch.save(self.G.state_dict(), os.path.join('checkpoints_16_200_final', 'Generator_fr_wrst_img'))\n",
        "              torch.save(self.D.state_dict(), os.path.join('checkpoints_16_200_final', 'Disc_fr_wrst_img'))\n",
        "          \n",
        "          if epoch == 50:\n",
        "              torch.save(self.G.state_dict(), os.path.join('checkpoints_16_200_final', 'Generator_fr_avg_img'))\n",
        "              torch.save(self.D.state_dict(), os.path.join('checkpoints_16_200_final', 'Disc_fr_avg_img'))\n",
        "\n",
        "      return loss_g_list, loss_triplet_list, loss_unsuper_list, loss_disc_list\n",
        "\n",
        "    def predict(D):\n",
        "      # D.load_state_dict(torch.load(os.path.join('checkpoints_16_200_exp', 'Discriminator')))\n",
        "      D = self.D.cuda()\n",
        "      train_loader = DataLoader(MnistUnlabel(), batch_size = 60000, shuffle = True, drop_last = True, num_workers = 4).__iter__()\n",
        "      x, y = next(train_loader)\n",
        "      x = x.cuda()\n",
        "      y = y.cuda()\n",
        "      feature_16_list = D(x)\n",
        "      train_x = feature_16_list.detach().cpu().numpy()\n",
        "      train_y = y.detach().cpu().numpy()\n",
        "      test_loader = DataLoader(MnistTest(), batch_size = 10000, shuffle = True, drop_last = True, num_workers = 4).__iter__()\n",
        "      x, y = next(test_loader)\n",
        "      x = x.cuda()\n",
        "      y = y.cuda()\n",
        "      feature_16_list = D(x)\n",
        "      test_x = feature_16_list.detach().cpu().numpy()\n",
        "      test_y = y.detach().cpu().numpy()\n",
        "      print(train_x.shape, test_x.shape, train_y.shape, test_y.shape)\n",
        "      \n",
        "      neigh = KNeighborsClassifier(n_neighbors = 9)\n",
        "      neigh.fit(train_x, train_y)\n",
        "      pred_y = neigh.predict(test_x)\n",
        "      print(accuracy_score(test_y, pred_y))\n",
        "      # print(precision_score(test_y, pred_y, average = 'macro'))\n",
        "      # Y = cdist(test_x,train_x)\n",
        "      # ind = np.argsort(Y,axis=1)\n",
        "      # prec = 0.0;\n",
        "      # acc = [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0];\n",
        "      # # calculating statistics\n",
        "      # for k in range(np.shape(test_y)[0]):\n",
        "      #     class_values = train_y[ind[k,:]]\n",
        "      #     y_true = (test_y[k] == class_values)\n",
        "      #     y_scores = np.arange(y_true.shape[0],0,-1)\n",
        "      #     ap = average_precision_score(y_true, y_scores)\n",
        "      #     prec = prec + ap\n",
        "      #     for n in range(len(acc)):\n",
        "      #         a = class_values[0:(n+1)]\n",
        "      #         counts = np.bincount(a)\n",
        "      #         b = np.where(counts==np.max(counts))[0]\n",
        "      #         if test_y[k] in b:\n",
        "      #             acc[n] = acc[n] + (1.0/float(len(b)))\n",
        "      # prec = prec/float(np.shape(test_x)[0])\n",
        "      # acc= [x / float(np.shape(test_x)[0]) for x in acc]\n",
        "      # print(\"Final results: \")\n",
        "      # print(\"mAP value: %.4f \"% prec)\n",
        "      # for k in range(len(acc)):\n",
        "      #     print(\"Accuracy for %d - NN: %.2f %%\" % (k+1,100*acc[k]) )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omLys6kxolrQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class params():\n",
        "  description = 'PyTorch Improved GAN'\n",
        "  batch_size = 300           \n",
        "  epochs = 100\n",
        "  lr = 0.003\n",
        "  momentum = 0.5\n",
        "  cuda = False\n",
        "  seed = 1\n",
        "  log_interval = 100\n",
        "  eval_interval = 1\n",
        "  unlabel_weight = 1\n",
        "  logdir = './logfile'\n",
        "  savedir = './checkpoints' \n",
        "  load_saved = True\n",
        "  save = True\n",
        "  generator_save = 'Generator'\n",
        "  discriminator_save = 'Discriminator'\n",
        "\n",
        "args = params()\n",
        "if torch.cuda.is_available():\n",
        "  args.cuda = True\n",
        "np.random.seed(args.seed)\n",
        "gan = ImprovedGAN(Generator(100), Discriminator(), MnistLabel(10, 60000), MnistUnlabel(), MnistTest(), args)\n",
        "loss_g, loss_triplet, loss_unsuper, loss_d = gan.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiblQOW92BU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "itr = []\n",
        "for i in range(100):\n",
        "  itr.append(i)\n",
        "plt1, = plt.plot(itr, loss_g, label = 'Generator')\n",
        "plt2, = plt.plot(itr, loss_d, label = 'Disriminator')\n",
        "# plt3, = plt.plot(itr, loss_triplet, label = 'Triplet Loss')\n",
        "plt.title(\"Loss \")\n",
        "plt.legend(handles=[plt1, plt2])\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMUIVHCzfvT7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(D):\n",
        "      D.load_state_dict(torch.load(os.path.join('checkpoints_16_200_final', 'Discriminator')))\n",
        "      D = D.cuda()\n",
        "      train_loader = DataLoader(MnistUnlabel(), batch_size = 60000, shuffle = True, drop_last = True, num_workers = 4).__iter__()\n",
        "      x, y = next(train_loader)\n",
        "      x = x.cuda()\n",
        "      y = y.cuda()\n",
        "      feature_16_list = D(x)\n",
        "      train_x = feature_16_list.detach().cpu().numpy()\n",
        "      train_y = y.detach().cpu().numpy()\n",
        "      test_loader = DataLoader(MnistTest(), batch_size = 10000, shuffle = True, drop_last = True, num_workers = 4).__iter__()\n",
        "      x, y = next(test_loader)\n",
        "      x = x.cuda()\n",
        "      y = y.cuda()\n",
        "      feature_16_list = D(x)\n",
        "      test_x = feature_16_list.detach().cpu().numpy()\n",
        "      test_y = y.detach().cpu().numpy()\n",
        "      print(train_x.shape, test_x.shape, train_y.shape, test_y.shape)\n",
        "      \n",
        "      neigh = KNeighborsClassifier(n_neighbors = 9)\n",
        "      neigh.fit(train_x, train_y)\n",
        "      pred_y = neigh.predict(test_x)\n",
        "      print(accuracy_score(test_y, pred_y))\n",
        "      print(precision_score(test_y, pred_y, average = 'macro'))\n",
        "      Y = cdist(test_x,train_x)\n",
        "      ind = np.argsort(Y,axis=1)\n",
        "      prec = 0.0;\n",
        "      acc = [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0];\n",
        "      # calculating statistics\n",
        "      for k in range(np.shape(test_y)[0]):\n",
        "          class_values = train_y[ind[k,:]]\n",
        "          y_true = (test_y[k] == class_values)\n",
        "          y_scores = np.arange(y_true.shape[0],0,-1)\n",
        "          ap = average_precision_score(y_true, y_scores)\n",
        "          prec = prec + ap\n",
        "          for n in range(len(acc)):\n",
        "              a = class_values[0:(n+1)]\n",
        "              counts = np.bincount(a)\n",
        "              b = np.where(counts==np.max(counts))[0]\n",
        "              if test_y[k] in b:\n",
        "                  acc[n] = acc[n] + (1.0/float(len(b)))\n",
        "      prec = prec/float(np.shape(test_x)[0])\n",
        "      acc= [x / float(np.shape(test_x)[0]) for x in acc]\n",
        "      print(\"Final results: \")\n",
        "      print(\"mAP value: %.4f \"% prec)\n",
        "      for k in range(len(acc)):\n",
        "          print(\"Accuracy for %d - NN: %.2f %%\" % (k+1,100*acc[k]) )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buQrz7IlJ8YI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict(D = Discriminator())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCcxQ0nQDqUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_images(G):\n",
        "      \n",
        "      G.load_state_dict(torch.load(os.path.join('checkpoints_16_200_final', 'Generator')))\n",
        "      G = G.cuda()\n",
        "      bst_image = G(2)[1]\n",
        "      bst_image = bst_image.detach().cpu().numpy()\n",
        "      bst_image = np.reshape(bst_image, (28, 28))\n",
        "      plt.imshow(bst_image)\n",
        "      plt.show()\n",
        "           \n",
        "      G.load_state_dict(torch.load(os.path.join('checkpoints_16_200_v2', 'Generator_fr_wrst_img')))\n",
        "      G = G.cuda()\n",
        "      wrst_image = G(2)[0]\n",
        "      wrst_image = wrst_image.detach().cpu().numpy()\n",
        "      wrst_image = np.reshape(wrst_image, (28, 28))\n",
        "      print(wrst_image.shape)\n",
        "      plt.imshow(wrst_image)\n",
        "      plt.show()\n",
        "      \n",
        "      G.load_state_dict(torch.load(os.path.join('checkpoints_16_200_v2', 'Generator_fr_avg_img')))\n",
        "      G = G.cuda()\n",
        "      avg_image = G(2)[1]\n",
        "      avg_image = avg_image.detach().cpu().numpy()\n",
        "      avg_image = np.reshape(avg_image, (28, 28))\n",
        "      plt.imshow(avg_image)\n",
        "      plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_WPppuQrGqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_images(G = Generator(100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxbqaqkIt8bu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}