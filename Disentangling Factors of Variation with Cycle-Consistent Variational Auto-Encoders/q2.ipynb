{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "q2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6-atapMHsUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pl3b6FRoILjX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd drive/My Drive/DL_3/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QcV1DGOYG83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorboardx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2ow_zAFoot-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from itertools import cycle\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "import h5py\n",
        "from itertools import cycle\n",
        "from collections import OrderedDict\n",
        "from scipy.io import loadmat\n",
        "import glob\n",
        "import os\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from tensorboardX import SummaryWriter\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import Variable\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "from torchvision.transforms import Compose, ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "from collections import defaultdict\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOWJqj5hovve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filenames = glob.glob('data/sprites/*.mat')\n",
        "filenames = sorted(filenames)\n",
        "split_file = filenames[len(filenames) - 1]\n",
        "print(split_file)\n",
        "filenames = filenames[0 : len(filenames) - 2]\n",
        "print(filenames)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_-mLlSi10mC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(split_file)\n",
        "f = loadmat(split_file)\n",
        "print(f.keys())\n",
        "print(f['trainidx'][0])\n",
        "print(f['testidx'][0])\n",
        "print(f['validx'][0])\n",
        "# print(f['sprites'].shape)\n",
        "# for j in range(0, 21):\n",
        "#   print(np.shape(f[f.get('sprites')[j][0]].value))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTUtihvm9r06",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dict = defaultdict()\n",
        "train_dict = { i : 1 for i in f['trainidx'][0]}\n",
        "print(train_dict)\n",
        "test_dict = defaultdict()\n",
        "test_dict = {i : 1 for i in f['testidx'][0]}\n",
        "print(test_dict)\n",
        "valid_dict = defaultdict()\n",
        "valid_dict = {i : 1 for i in f['validx'][0]}\n",
        "print(valid_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zw3-rCakDHXu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_filenames = []\n",
        "test_filenames = []\n",
        "valid_filenames = []\n",
        "file_list = []\n",
        "c = 0\n",
        "for i in filenames:\n",
        "  str = i[-7: -4]\n",
        "  # print(str)\n",
        "  if str[0] >= 'a' and str[0] <= 'z':\n",
        "    # print(str[2:])\n",
        "    str = int(str[2:])\n",
        "  elif str[0] == '_':\n",
        "    # print(str[1:])\n",
        "    str = int(str[1 :])\n",
        "  else:\n",
        "    str = int(str)\n",
        "  # print(str)\n",
        "  file_list.append(str)\n",
        "\n",
        "for i in file_list:\n",
        "  if i in train_dict:\n",
        "    train_filenames.append(filenames[c])\n",
        "  elif i in test_dict:\n",
        "    test_filenames.append(filenames[c])\n",
        "  elif i in valid_dict:\n",
        "    valid_filenames.append(filenames[c])\n",
        "  else:\n",
        "    print(type(i), type(train_dict[585]))\n",
        "  c += 1\n",
        "print(c)\n",
        "print(len(train_filenames))\n",
        "print(len(test_filenames))\n",
        "print(len(valid_filenames))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alSDfgK_piQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def construct_dictionary(filenames):\n",
        "  num = 0\n",
        "  data_dict = {}\n",
        "  for i in filenames[0: 100]:\n",
        "    f = h5py.File(i, 'r')\n",
        "    # print(np.shape(f.get('sprites')))\n",
        "    # arr = np.reshape(f[f.get('sprites')[4][0]].value,(7, 3, 60, 60))\n",
        "    for j in range(0, 21):\n",
        "      # print(np.shape(f[f.get('sprites')[j][0]].value))\n",
        "      k = np.shape(f[f.get('sprites')[j][0]].value)[0]\n",
        "      arr = np.reshape(f[f.get('sprites')[j][0]].value,(k, 3, 60, 60))\n",
        "      if num not in data_dict:\n",
        "        data_dict[num] =  arr\n",
        "      else:\n",
        "        arr2 = data_dict[num]\n",
        "        arr2 = np.concatenate([arr2, arr], 0)\n",
        "        data_dict[num] = arr2\n",
        "\n",
        "    num += 1\n",
        "  return data_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJBiNI97PYY8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dict = construct_dictionary(train_filenames)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ3_vQhNRqDb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_dict = construct_dictionary(test_filenames)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEhc1K-fTSTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for keys in data_dict:\n",
        "#   # print(keys)\n",
        "#   print(np.shape(data_dict[keys]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Wd6SiVh53-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils import data\n",
        "\n",
        "class Dataset():\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "  def __init__(self, data_dict, num_samples):\n",
        "        'Initialization'\n",
        "        self.data_dict = data_dict\n",
        "        self.num_samples = num_samples\n",
        "  def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return self.num_samples\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        key = (index // 178) \n",
        "        num_img = (index // 100)\n",
        "        # print(key)\n",
        "        arr = self.data_dict[key]\n",
        "        image = arr[num_img]\n",
        "        label = key\n",
        "        return image, random.SystemRandom().choice(self.data_dict[key]), label\n",
        "\n",
        "dataset = Dataset(data_dict, 178 * 100)\n",
        "loader = cycle(DataLoader(dataset, batch_size = 8, shuffle = True, num_workers=0, drop_last=True))\n",
        "print(dataset.data_dict.keys())\n",
        "\n",
        "image_batch, image_batch_2, labels_batch = next(loader)\n",
        "print(labels_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xmy8wzvHSCw4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class test_Dataset():\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "  def __init__(self, data_dict, num_samples):\n",
        "        'Initialization'\n",
        "        self.data_dict = data_dict\n",
        "        self.num_samples = num_samples\n",
        "  def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return self.num_samples\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        key = (index // 178) \n",
        "        num_img = (index // 100)\n",
        "        # print(key)\n",
        "        arr = self.data_dict[key]\n",
        "        image = arr[num_img]\n",
        "        label = key\n",
        "        return image, random.SystemRandom().choice(self.data_dict[key]), label\n",
        "\n",
        "test_dataset = test_Dataset(test_data_dict, 178 * 100)\n",
        "test_loader = cycle(DataLoader(test_dataset, batch_size = 1, shuffle = True, num_workers=0, drop_last=True))\n",
        "print(dataset.data_dict.keys())\n",
        "\n",
        "image_batch, image_batch_2, labels_batch = next(test_loader)\n",
        "print(labels_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE-e5Rrhh7Co",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_batch_2.size()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzqyArZ2srjY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Architecture taken from @ https://github.com/ananyahjha93/cycle-consistent-vae\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, style_dim, class_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels= 3, out_channels=16, kernel_size=5, stride=2, padding=1, bias=True)\n",
        "        self.conv_1_in = nn.InstanceNorm2d(num_features=16, track_running_stats=True)\n",
        "        self.relu_1 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=2, padding=1, bias=True)\n",
        "        self.conv_2_in = nn.InstanceNorm2d(num_features=32, track_running_stats=True)\n",
        "        self.relu_2 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=2, padding=1, bias=True)\n",
        "        self.conv_3_in = nn.InstanceNorm2d(num_features=64, track_running_stats=True)\n",
        "        self.relu_3 = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Style embeddings\n",
        "        self.style_mu = nn.Linear(in_features= 2304, out_features=style_dim, bias=True)\n",
        "        self.style_logvar = nn.Linear(in_features= 2304, out_features=style_dim, bias=True)\n",
        "\n",
        "        # Class embeddings\n",
        "        self.class_output = nn.Linear(in_features=2304, out_features=class_dim, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv_1_in(x)\n",
        "        x = self.relu_1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv_2_in(x)\n",
        "        x = self.relu_2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv_3_in(x)\n",
        "        x = self.relu_3(x)\n",
        "        x = x.view(x.size(0), x.size(1) * x.size(2) * x.size(3))\n",
        "\n",
        "        style_embeddings_mu = self.style_mu(x)\n",
        "        style_embeddings_logvar = self.style_logvar(x)\n",
        "        class_embeddings = self.class_output(x)\n",
        "\n",
        "        return style_embeddings_mu, style_embeddings_logvar, class_embeddings\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, style_dim, class_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        # Style embeddings input\n",
        "        self.style_input = nn.Linear(in_features=style_dim, out_features=2304, bias=True)\n",
        "\n",
        "        # Class embeddings input\n",
        "        self.class_input = nn.Linear(in_features=class_dim, out_features=2304, bias=True)\n",
        "\n",
        "        self.deconv1 = nn.ConvTranspose2d(in_channels=128, out_channels=32, kernel_size=4, stride=2, padding=0, bias=True)\n",
        "        self.deconv1_in = nn.InstanceNorm2d(num_features=32, track_running_stats=True)\n",
        "        self.deconv1_relu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "\n",
        "        self.deconv2 = nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=4, stride=2, padding=0, bias=True)\n",
        "        self.deconv2_in = nn.InstanceNorm2d(num_features= 16, track_running_stats=True)\n",
        "        self.deconv2_relu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "\n",
        "        self.deconv3 = nn.ConvTranspose2d(in_channels=16, out_channels= 3, kernel_size=4, stride=2, padding=1, bias=True)\n",
        "        self.sigmoid_output = nn.Sigmoid()\n",
        "       \n",
        "    def forward(self, style_embeddings, class_embeddings):\n",
        "        style_embeddings = F.leaky_relu_(self.style_input(style_embeddings), negative_slope=0.2)\n",
        "        class_embeddings = F.leaky_relu_(self.class_input(class_embeddings), negative_slope=0.2)\n",
        "\n",
        "        x = torch.cat((style_embeddings, class_embeddings), dim=1)\n",
        "        x = x.view(x.size(0), 128, 6, 6)\n",
        "        x = self.deconv1(x)\n",
        "        x = self.deconv1_in(x)\n",
        "        x = self.deconv1_relu(x)\n",
        "\n",
        "        x = self.deconv2(x)\n",
        "        x = self.deconv2_in(x)\n",
        "        x = self.deconv2_relu(x)\n",
        "\n",
        "        x = self.deconv3(x)\n",
        "        x = self.sigmoid_output(x)\n",
        "\n",
        "        return x\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDA6tmiHVc_X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ags():\n",
        "  def __init__(self):\n",
        " \n",
        "# add arguments\n",
        "  \n",
        "    self.cuda = True\n",
        "    self.batch_size = 16\n",
        "    self.num_channels = 3\n",
        "    self.initial_learning_rate = 0.0001\n",
        "    self.style_dim = 64\n",
        "    self.class_dim = 64\n",
        "    self.num_classes = 500\n",
        "    self.reconstruction_coef = 2\n",
        "    self.reverse_cycle_coef = 10.\n",
        "    self.kl_divergence_coef = 3.\n",
        "    self.beta_1 = 0.9\n",
        "    self.beta_2 = 0.999\n",
        "    self.log_file = 'log.txt'\n",
        "    self.image_size = 60\n",
        "    self.load_saved = False\n",
        "    self.encoder_save = 'encoder'\n",
        "    self.decoder_save = 'decoder'\n",
        "    self.start_epoch = 0\n",
        "    self.end_epoch = 10\n",
        "\n",
        "  \n",
        "FLAGS = ags()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8M9K3pWHIbQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform_config = Compose([ToTensor()])\n",
        "\n",
        "\n",
        "def mse_loss(input, target):\n",
        "    return torch.sum((input - target).pow(2)) / input.data.nelement()\n",
        "\n",
        "\n",
        "def l1_loss(input, target):\n",
        "    return torch.sum(torch.abs(input - target)) / input.data.nelement()\n",
        "\n",
        "\n",
        "def reparameterize(training, mu, logvar):\n",
        "    if training:\n",
        "        std = logvar.mul(0.5).exp_()\n",
        "        eps = Variable(std.data.new(std.size()).normal_())\n",
        "        return eps.mul(std).add_(mu)\n",
        "    else:\n",
        "        return mu\n",
        "\n",
        "\n",
        "def weights_init(layer):\n",
        "    if isinstance(layer, nn.Conv2d):\n",
        "        layer.weight.data.normal_(0.0, 0.05)\n",
        "        layer.bias.data.zero_()\n",
        "    elif isinstance(layer, nn.BatchNorm2d):\n",
        "        layer.weight.data.normal_(1.0, 0.02)\n",
        "        layer.bias.data.zero_()\n",
        "    elif isinstance(layer, nn.Linear):\n",
        "        layer.weight.data.normal_(0.0, 0.05)\n",
        "        layer.bias.data.zero_()\n",
        "\n",
        "def train(FLAGS):\n",
        "    \"\"\"\n",
        "    model definition\n",
        "    \"\"\"\n",
        "    encoder = Encoder(style_dim=FLAGS.style_dim, class_dim=FLAGS.class_dim)\n",
        "    encoder.apply(weights_init)\n",
        "\n",
        "    decoder = Decoder(style_dim=FLAGS.style_dim, class_dim=FLAGS.class_dim)\n",
        "    decoder.apply(weights_init)\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "    style_latent_space = torch.FloatTensor(FLAGS.batch_size, FLAGS.style_dim)\n",
        "\n",
        "    \"\"\"\n",
        "    loss definitions\n",
        "    \"\"\"\n",
        "    cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    '''\n",
        "    add option to run on GPU\n",
        "    '''\n",
        "    if FLAGS.cuda:\n",
        "        encoder.cuda()\n",
        "        decoder.cuda()\n",
        "\n",
        "        cross_entropy_loss.cuda()\n",
        "\n",
        "        X_1 = X_1.cuda()\n",
        "        X_2 = X_2.cuda()\n",
        "        X_3 = X_3.cuda()\n",
        "\n",
        "        style_latent_space = style_latent_space.cuda()\n",
        "\n",
        "    \"\"\"\n",
        "    optimizer and scheduler definition\n",
        "    \"\"\"\n",
        "    auto_encoder_optimizer = optim.Adam(\n",
        "        list(encoder.parameters()) + list(decoder.parameters()),\n",
        "        lr=FLAGS.initial_learning_rate,\n",
        "        betas=(FLAGS.beta_1, FLAGS.beta_2)\n",
        "    )\n",
        "\n",
        "    reverse_cycle_optimizer = optim.Adam(\n",
        "        list(encoder.parameters()),\n",
        "        lr=FLAGS.initial_learning_rate,\n",
        "        betas=(FLAGS.beta_1, FLAGS.beta_2)\n",
        "    )\n",
        "\n",
        "    # divide the learning rate by a factor of 10 after 80 epochs\n",
        "    auto_encoder_scheduler = optim.lr_scheduler.StepLR(auto_encoder_optimizer, step_size=80, gamma=0.1)\n",
        "    reverse_cycle_scheduler = optim.lr_scheduler.StepLR(reverse_cycle_optimizer, step_size=80, gamma=0.1)\n",
        "\n",
        "    \"\"\"\n",
        "    training\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available() and not FLAGS.cuda:\n",
        "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
        "\n",
        "    if not os.path.exists('checkpoints'):\n",
        "        os.makedirs('checkpoints')\n",
        "\n",
        "    if not os.path.exists('reconstructed_images'):\n",
        "        os.makedirs('reconstructed_images')\n",
        "\n",
        "    # load_saved is false when training is started from 0th iteration\n",
        "    if not FLAGS.load_saved:\n",
        "        with open(FLAGS.log_file, 'w') as log:\n",
        "            log.write('Epoch\\tIteration\\tReconstruction_loss\\tKL_divergence_loss\\tReverse_cycle_loss\\n')\n",
        "\n",
        "    # load data set and create data loader instance\n",
        "    print('Loading stripes paired dataset...')\n",
        "    \n",
        "    loader = cycle(DataLoader(dataset, batch_size=FLAGS.batch_size, shuffle=True, num_workers=0, drop_last=True))\n",
        "\n",
        "    # initialize summary writer\n",
        "    writer = SummaryWriter()\n",
        "\n",
        "    for epoch in range(FLAGS.start_epoch, FLAGS.end_epoch):\n",
        "        print('')\n",
        "        print('Epoch #' + str(epoch) + '..........................................................................')\n",
        "\n",
        "        # update the learning rate scheduler\n",
        "        auto_encoder_scheduler.step()\n",
        "        reverse_cycle_scheduler.step()\n",
        "\n",
        "        for iteration in range(int(len(dataset) / FLAGS.batch_size)):\n",
        "            # A. run the auto-encoder reconstruction\n",
        "            image_batch_1, image_batch_2, _ = next(loader)\n",
        "\n",
        "            auto_encoder_optimizer.zero_grad()\n",
        "\n",
        "            X_1.copy_(image_batch_1)\n",
        "            X_2.copy_(image_batch_2)\n",
        "\n",
        "            style_mu_1, style_logvar_1, class_latent_space_1 = encoder(Variable(X_1))\n",
        "            style_latent_space_1 = reparameterize(training=True, mu=style_mu_1, logvar=style_logvar_1)\n",
        "\n",
        "            kl_divergence_loss_1 = FLAGS.kl_divergence_coef * (\n",
        "                - 0.5 * torch.sum(1 + style_logvar_1 - style_mu_1.pow(2) - style_logvar_1.exp())\n",
        "            )\n",
        "            kl_divergence_loss_1 /= (FLAGS.batch_size * FLAGS.num_channels * FLAGS.image_size * FLAGS.image_size)\n",
        "            kl_divergence_loss_1.backward(retain_graph=True)\n",
        "\n",
        "            style_mu_2, style_logvar_2, class_latent_space_2 = encoder(Variable(X_2))\n",
        "            style_latent_space_2 = reparameterize(training=True, mu=style_mu_2, logvar=style_logvar_2)\n",
        "\n",
        "            kl_divergence_loss_2 = FLAGS.kl_divergence_coef * (\n",
        "                - 0.5 * torch.sum(1 + style_logvar_2 - style_mu_2.pow(2) - style_logvar_2.exp())\n",
        "            )\n",
        "            kl_divergence_loss_2 /= (FLAGS.batch_size * FLAGS.num_channels * FLAGS.image_size * FLAGS.image_size)\n",
        "            kl_divergence_loss_2.backward(retain_graph=True)\n",
        "\n",
        "            reconstructed_X_1 = decoder(style_latent_space_1, class_latent_space_2)\n",
        "            reconstructed_X_2 = decoder(style_latent_space_2, class_latent_space_1)\n",
        "\n",
        "            reconstruction_error_1 = FLAGS.reconstruction_coef * mse_loss(reconstructed_X_1, Variable(X_1))\n",
        "            reconstruction_error_1.backward(retain_graph=True)\n",
        "\n",
        "            reconstruction_error_2 = FLAGS.reconstruction_coef * mse_loss(reconstructed_X_2, Variable(X_2))\n",
        "            reconstruction_error_2.backward()\n",
        "\n",
        "            reconstruction_error = (reconstruction_error_1 + reconstruction_error_2) / FLAGS.reconstruction_coef\n",
        "            kl_divergence_error = (kl_divergence_loss_1 + kl_divergence_loss_2) / FLAGS.kl_divergence_coef\n",
        "\n",
        "            auto_encoder_optimizer.step()\n",
        "\n",
        "            # B. reverse cycle\n",
        "            image_batch_1, _, __ = next(loader)\n",
        "            image_batch_2, _, __ = next(loader)\n",
        "\n",
        "            reverse_cycle_optimizer.zero_grad()\n",
        "\n",
        "            X_1.copy_(image_batch_1)\n",
        "            X_2.copy_(image_batch_2)\n",
        "\n",
        "            style_latent_space.normal_(0., 1.)\n",
        "\n",
        "            _, __, class_latent_space_1 = encoder(Variable(X_1))\n",
        "            _, __, class_latent_space_2 = encoder(Variable(X_2))\n",
        "\n",
        "            reconstructed_X_1 = decoder(Variable(style_latent_space), class_latent_space_1.detach())\n",
        "            reconstructed_X_2 = decoder(Variable(style_latent_space), class_latent_space_2.detach())\n",
        "\n",
        "            style_mu_1, style_logvar_1, _ = encoder(reconstructed_X_1)\n",
        "            style_latent_space_1 = reparameterize(training=False, mu=style_mu_1, logvar=style_logvar_1)\n",
        "\n",
        "            style_mu_2, style_logvar_2, _ = encoder(reconstructed_X_2)\n",
        "            style_latent_space_2 = reparameterize(training=False, mu=style_mu_2, logvar=style_logvar_2)\n",
        "\n",
        "            reverse_cycle_loss = FLAGS.reverse_cycle_coef * l1_loss(style_latent_space_1, style_latent_space_2)\n",
        "            reverse_cycle_loss.backward()\n",
        "            reverse_cycle_loss /= FLAGS.reverse_cycle_coef\n",
        "\n",
        "            reverse_cycle_optimizer.step()\n",
        "\n",
        "            if (iteration + 1) % 10 == 0:\n",
        "                print('')\n",
        "                print('Epoch #' + str(epoch))\n",
        "                print('Iteration #' + str(iteration))\n",
        "\n",
        "                print('')\n",
        "                print('Reconstruction loss: ' + str(reconstruction_error.data.storage().tolist()[0]))\n",
        "                print('KL-Divergence loss: ' + str(kl_divergence_error.data.storage().tolist()[0]))\n",
        "                print('Reverse cycle loss: ' + str(reverse_cycle_loss.data.storage().tolist()[0]))\n",
        "\n",
        "            # write to log\n",
        "            with open(FLAGS.log_file, 'a') as log:\n",
        "                log.write('{0}\\t{1}\\t{2}\\t{3}\\t{4}\\n'.format(\n",
        "                    epoch,\n",
        "                    iteration,\n",
        "                    reconstruction_error.data.storage().tolist()[0],\n",
        "                    kl_divergence_error.data.storage().tolist()[0],\n",
        "                    reverse_cycle_loss.data.storage().tolist()[0]\n",
        "                ))\n",
        "\n",
        "            # write to tensorboard\n",
        "            writer.add_scalar('Reconstruction loss', reconstruction_error.data.storage().tolist()[0],\n",
        "                              epoch * (int(len(dataset) / FLAGS.batch_size) + 1) + iteration)\n",
        "            writer.add_scalar('KL-Divergence loss', kl_divergence_error.data.storage().tolist()[0],\n",
        "                              epoch * (int(len(dataset) / FLAGS.batch_size) + 1) + iteration)\n",
        "            writer.add_scalar('Reverse cycle loss', reverse_cycle_loss.data.storage().tolist()[0],\n",
        "                              epoch * (int(len(dataset) / FLAGS.batch_size) + 1) + iteration)\n",
        "\n",
        "        # save model after every 5 epochs\n",
        "        if (epoch + 1) % 5 == 0 or (epoch + 1) == FLAGS.end_epoch:\n",
        "            torch.save(encoder.state_dict(), os.path.join('checkpoints', FLAGS.encoder_save))\n",
        "            torch.save(decoder.state_dict(), os.path.join('checkpoints', FLAGS.decoder_save))\n",
        "\n",
        "            \n",
        "            \n",
        "\n",
        "def plot_style_grid_matrix(FLAGS):\n",
        "\n",
        "\n",
        "  encoder = Encoder(style_dim=FLAGS.style_dim, class_dim=FLAGS.class_dim)\n",
        "   \n",
        "  decoder = Decoder(style_dim=FLAGS.style_dim, class_dim=FLAGS.class_dim)\n",
        "  \"\"\"\n",
        "  save reconstructed images and style swapped image generations to check progress\n",
        "            \"\"\"\n",
        "  \"\"\"\n",
        "    variable definition\n",
        "    \"\"\"\n",
        "\n",
        "  X_1 = torch.FloatTensor(FLAGS.batch_size, FLAGS.num_channels, FLAGS.image_size, FLAGS.image_size)\n",
        "  X_2 = torch.FloatTensor(FLAGS.batch_size, FLAGS.num_channels, FLAGS.image_size, FLAGS.image_size)\n",
        " \n",
        "  # load saved models if load_saved flag is true\n",
        "  if FLAGS.load_saved:\n",
        "      encoder.load_state_dict(torch.load(os.path.join('checkpoints', FLAGS.encoder_save)))\n",
        "      decoder.load_state_dict(torch.load(os.path.join('checkpoints', FLAGS.decoder_save)))\n",
        "  \n",
        "\n",
        "\n",
        "  if FLAGS.cuda:\n",
        "        encoder.cuda()\n",
        "        decoder.cuda()\n",
        "\n",
        "        \n",
        "\n",
        "        X_1 = X_1.cuda()\n",
        "        X_2 = X_2.cuda()\n",
        "        \n",
        "  \n",
        " \n",
        "  image_batch_1, _, _ = next(test_loader)\n",
        "  image_batch_2, _, _ = next(test_loader)\n",
        "  \n",
        "  \n",
        "  X_1.copy_(image_batch_1)\n",
        "  X_2.copy_(image_batch_2)\n",
        "  \n",
        "\n",
        "  style_mu_1, style_logvar_1, _ = encoder(Variable(X_1))\n",
        "  _, __, class_latent_space_2 = encoder(Variable(X_2))\n",
        "  \n",
        "\n",
        "  style_latent_space_1 = reparameterize(training=False, mu=style_mu_1, logvar=style_logvar_1)\n",
        " \n",
        "  # print(np.array(style_latent_space_1.detach().cpu()).shape, np.array(class_latent_space_2.detach().cpu()).shape)\n",
        "  style_latent_nparray = np.array(style_latent_space_1.detach().cpu())\n",
        "  # print(arr[0] == arr[1], arr[0] == arr[2])\n",
        "\n",
        "  # reconstructed_X_1_2 = decoder(style_latent_space_1, class_latent_space_2)\n",
        "  # reconstructed_X_3_2 = decoder(style_latent_space_3, class_latent_space_2)\n",
        "  k = style_latent_nparray.shape[0]\n",
        "  reconstructed_images = np.zeros((k, 60, 60, 3))\n",
        "  for i in range(0, k):\n",
        "    style_latent_temp = np.zeros((k, FLAGS.style_dim),dtype = 'float32')\n",
        "    for j in range(0, k):\n",
        "      style_latent_temp[j] = style_latent_nparray[i]\n",
        "\n",
        "    \n",
        "    if FLAGS.cuda:\n",
        "      style_latent_temp = torch.from_numpy(style_latent_temp)\n",
        "      style_latent_temp = style_latent_temp.cuda()\n",
        "    reconstructed_x = decoder(style_latent_temp, class_latent_space_2)\n",
        "    reconstructed_xx = np.transpose(reconstructed_x.cpu().data.numpy(), (0, 3, 2, 1))\n",
        "    if i == 0:\n",
        "      reconstructed_images = np.copy(reconstructed_xx)\n",
        "    else:\n",
        "      reconstructed_images = np.concatenate([reconstructed_images, reconstructed_xx], 0)\n",
        "  \n",
        "  image_batch_2 = np.array(image_batch_2)\n",
        "  print(type(image_batch_2))\n",
        "  image_batch_2 = np.transpose(image_batch_2, (0, 3, 2, 1))\n",
        "  reconstructed_images = np.concatenate([image_batch_2, reconstructed_images], 0)\n",
        "  print(reconstructed_images.shape)\n",
        "  image_batch_1 = np.transpose(image_batch_1.numpy(), (0, 3, 2, 1))\n",
        "  blank_img = np.zeros((60, 60, 3))\n",
        "  image_list = []\n",
        "  image_list.append(blank_img)\n",
        "  j = 0\n",
        "  for i in range(reconstructed_images.shape[0]):\n",
        "    if i % 8 == 0 and j != 8 and i != 0:\n",
        "      image_list.append(image_batch_1[j])\n",
        "      \n",
        "      j += 1\n",
        "    image_list.append(reconstructed_images[i])\n",
        "  print(len(image_list))\n",
        "  fig = plt.figure(figsize=(12., 12.))\n",
        "  grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
        "                  nrows_ncols=(9, 9),  # creates 2x2 grid of axes\n",
        "                  axes_pad=0.01,  # pad between axes in inch.\n",
        "                  )\n",
        "  # print(label1)\n",
        "  for ax, im in zip(grid, image_list):\n",
        "      # Iterating over the grid returns the Axes.\n",
        "      ax.imshow(im)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  # print(reconstructed_images[0] == reconstructed_images[255], reconstructed_images[0] == reconstructed_images[128])\n",
        "\n",
        "\n",
        "      \n",
        "def linear_interpolation_line(FLAGS):\n",
        "  encoder = Encoder(style_dim=FLAGS.style_dim, class_dim=FLAGS.class_dim)   \n",
        "  decoder = Decoder(style_dim=FLAGS.style_dim, class_dim=FLAGS.class_dim)\n",
        "  \n",
        "  X_1 = torch.FloatTensor(FLAGS.batch_size, FLAGS.num_channels, FLAGS.image_size, FLAGS.image_size)\n",
        "  X_2 = torch.FloatTensor(FLAGS.batch_size, FLAGS.num_channels, FLAGS.image_size, FLAGS.image_size)\n",
        " \n",
        "  # load saved models if load_saved flag is true\n",
        "  if FLAGS.load_saved:\n",
        "      encoder.load_state_dict(torch.load(os.path.join('checkpoints', FLAGS.encoder_save)))\n",
        "      decoder.load_state_dict(torch.load(os.path.join('checkpoints', FLAGS.decoder_save)))\n",
        " \n",
        "  if FLAGS.cuda:\n",
        "        encoder.cuda()\n",
        "        decoder.cuda()\n",
        "        X_1 = X_1.cuda()\n",
        "        X_2 = X_2.cuda() \n",
        "  image_batch_1, _ ,_  = next(test_loader)\n",
        "  image_batch_2, _, _ = next(test_loader)\n",
        "  \n",
        "  X_1.copy_(image_batch_1)\n",
        "  X_2.copy_(image_batch_2)\n",
        "\n",
        "  style_mu_1, style_logvar_1, class_latent_space_1 = encoder(Variable(X_1))\n",
        "  style_mu_2, style_logvar_2, class_latent_space_2 = encoder(Variable(X_2))\n",
        "  \n",
        "  style_latent_space_1 = reparameterize(training= False, mu = style_mu_1, logvar = style_logvar_1)\n",
        "  style_latent_space_2 = reparameterize(training = False, mu = style_mu_2, logvar = style_logvar_2)\n",
        "  image_batch_1 = np.transpose(image_batch_1.numpy(), (0, 3, 2, 1))\n",
        "  image_batch_2 = np.transpose(image_batch_2.numpy(), (0, 3, 2, 1))\n",
        "  style_latent_nparray1 = np.array(style_latent_space_1.detach().cpu())\n",
        "  style_latent_nparray2 = np.array(style_latent_space_2.detach().cpu())\n",
        "  class_latent_nparray2 = np.array(class_latent_space_2.detach().cpu())\n",
        "  class_latent_nparray1 = np.array(class_latent_space_1.detach().cpu())\n",
        "  reconstructed_images = np.zeros((8, 8, 60, 60, 3), dtype = 'float32')\n",
        "  class_latent_spaces = np.zeros((8, 8, 1, 64), dtype = 'float32')\n",
        "  style_latent_spaces = np.zeros((8, 8, 1, 64), dtype = 'float32')\n",
        "  steps = 8\n",
        "  for k in range(8):\n",
        "    new_class_latent_space = (class_latent_nparray2 - class_latent_nparray1) *  k / steps + class_latent_nparray1\n",
        "    style_latent_spaces[0][k] = style_latent_nparray1\n",
        "    class_latent_spaces[0][k] = new_class_latent_space\n",
        "    # x = torch.from_numpy(new_class_latent_space)\n",
        "    # class_latent_space = x.cuda()\n",
        "\n",
        "    \n",
        "    # recons = decoder(style_latent_space_1, class_latent_space)\n",
        "    # print(recons)\n",
        "    # reconstructed_images[k] = np.transpose(recons.detach().cpu().data.numpy(), (0, 3, 2, 1))[0]\n",
        "  for k in range(7, -1, -1):\n",
        "    new_class_spaces = (class_latent_nparray1 - class_latent_nparray2) * (7 - k)/steps + class_latent_nparray2\n",
        "    style_latent_spaces[7][k] = style_latent_nparray2\n",
        "    class_latent_spaces[7][k] = new_class_spaces\n",
        "  steps = 6\n",
        "  for k in range(0, 8):\n",
        "    for j in range(1, 7):\n",
        "      new_style_spaces = (style_latent_spaces[7][k] - style_latent_spaces[0][k])  * j / steps + style_latent_spaces[0][k]\n",
        "      style_latent_spaces[j][k] = new_style_spaces\n",
        "      class_latent_spaces[j][k] = class_latent_spaces[j][7]\n",
        "  for j in range(0, 8):\n",
        "    for k in range(0, 8):\n",
        "      # print(np.shape(style_latent_spaces[j][k]))\n",
        "      stl = torch.from_numpy(style_latent_spaces[j][k])\n",
        "      cls = torch.from_numpy(class_latent_spaces[j][k])\n",
        "      stl = stl.cuda()\n",
        "      cls = cls.cuda()\n",
        "      img = decoder(stl, cls)\n",
        "      reconstructed_images[j][k] = np.transpose(img.detach().cpu().data.numpy(), (0, 3, 2, 1))[0]\n",
        "      # print(np.shape(reconstructed_images[j][k]))\n",
        "  # print(np.shape(image_batch_1[0]))\n",
        "  reconstructed_images[0][0] = image_batch_1[0]\n",
        "  reconstructed_images[7][7] = image_batch_2[0]\n",
        "\n",
        "\n",
        "  image_list = []\n",
        "  \n",
        "  for i in range(reconstructed_images.shape[0]):\n",
        "    for j in range(reconstructed_images.shape[1]):\n",
        "      image_list.append(reconstructed_images[i][j])\n",
        "    \n",
        "  # print(len(image_list))\n",
        "  fig = plt.figure(figsize=(10., 10.))\n",
        "  grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
        "                  nrows_ncols=(8, 8),  # creates 2x2 grid of axes\n",
        "                  axes_pad=0.01,  # pad between axes in inch.\n",
        "                  )\n",
        "  # print(label1)\n",
        "  for ax, im in zip(grid, image_list):\n",
        "      # Iterating over the grid returns the Axes.\n",
        "      ax.imshow(im)\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmD_SuszLrrT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "F1 = ags()\n",
        "F1.load_saved = True\n",
        "F1.batch_size = 1\n",
        "linear_interpolation_line(F1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHXODBkWX6wF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "F2 = ags()\n",
        "F2.load_saved = True\n",
        "F2.batch_size = 8\n",
        "plot_style_grid_matrix(F2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__bD21iCbIFn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Encoder(64, 64)\n",
        "encoder.parameters()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYMmJSbofWQZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "np.arange(1, 8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0EHGJkRF1BD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}